# Ollama Model Server Automation

This repository automates building and publishing Docker images for Ollama model servers with preloaded models.

You can choose one of three ways to use it:

1. **GitHub Actions** ‚Äî Quick and easy; suitable for small models and embedding models (limited by GitHub runner storage).
2. **VS Code Tasks** ‚Äî Convenient from your IDE; more flexible.
3. **Manual Script Execution** ‚Äî Full control, ideal for large models.

Built images are pushed to **Docker Hub** and **AWS Elastic Container Registry (ECR)**.

---

## üìÅ Folder Structure

- `model_metadata/` ‚Äî YAML files describing each model's name, tag, memory, license, etc.
- `.github/workflows/` ‚Äî GitHub Actions workflow for build/test/push.
- `scripts/` ‚Äî Shell and PowerShell scripts for building/testing/pushing.
- `ollama/` ‚Äî Dockerfile and Ollama server setup.

---

## üß† Model Metadata Example

Save this in `model_metadata/gemma-7b.yaml`:

```yaml
name: gemma-7b
tag: latest
memory:
  model_size: 7GiB
  min: 8GiB
  recommended: 12GiB
license: apache-2.0
```

---

## üöÄ Options for Running the Pipeline

### üîπ 1. GitHub Actions (Manual Dispatch)

This is the easiest method for small or embedding models.

1. Go to **Actions** tab in GitHub.
2. Select `Build, Push and Test Ollama Model Image`.
3. Click **Run workflow**, and choose the `model_file`, e.g. `gemma2_2b.yaml`.

This will:

- Build and preload the model image.
- Test for model presence.
- Push to Docker Hub and AWS ECR.

---

### üîπ 2. VS Code Tasks (Large Models)

Use this if you want to avoid GitHub storage limits.

Open the command palette (`Ctrl+Shift+P`) ‚Üí **Run Task** ‚Üí choose one of:

- `build_ollama_single_model_server`
- `inspect_ollama_single_model_server`
- `push_ollama_single_model_server_to_aws`
- `push_ollama_single_model_server_to_dockerhub`
- `full_pipeline` (runs all the above in order)

Make sure to configure your VS Code `settings.json`:

```json
{
  "aws.region": "ca-central-1",
  "docker.hub.namespace": "your-dockerhub-username"
}
```

---

### üîπ 3. Manual Shell Script Execution

You can also run any of the scripts directly:

```bash
./scripts/build_ollama_single_model_server.sh model_metadata/gemma-7b.yaml
./scripts/inspect_ollama_single_model_server.sh model_metadata/gemma-7b.yaml
./scripts/push_ollama_single_model_server_to_aws.sh model_metadata/gemma-7b.yaml
./scripts/push_ollama_single_model_server_to_dockerhub.sh model_metadata/gemma-7b.yaml
```

---

## üß™ Run the Image

### Standalone Ollama Server

```yaml
version: '3.8'
services:
  ollama:
    image: your-dockerhub-username/ollama-server:gemma2-2b
    ports:
      - "11434:11434"
```

---

### With Open Web UI

```yaml
version: '3.8'
services:
  ollama:
    image: your-dockerhub-username/ollama-server:gemma2-2b
    ports:
      - "11434:11434"
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            capabilities: ["gpu"]
            count: all

  webui:
    image: ghcr.io/open-webui/open-webui:main
    ports:
      - "3000:3000"
    environment:
      - OLLAMA_API_BASE_URL=http://ollama:11434
    depends_on:
      - ollama
```

---

## üõ†Ô∏è Requirements

- [`yq`](https://github.com/mikefarah/yq) ‚Äî used for parsing YAML in GitHub workflows
- Docker
- AWS CLI (for ECR pushing)
- GitHub Secrets (for GitHub Actions):

  - `AWS_ACCESS_KEY_ID`
  - `AWS_SECRET_ACCESS_KEY`
  - `DOCKER_USERNAME`
  - `DOCKER_TOKEN`

---

## ü™™ License

MIT
